# -*- coding: utf-8 -*-
"""test_results.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bXe5gShm0hpjv2cjJUOVuzUPVgr9eUUw
"""

# Libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import datasets, models, transforms
from torchvision.datasets import ImageFolder
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader
import torch.backends.cudnn as cudnn

import matplotlib.pyplot as plt
import os

import sklearn.metrics
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay

cudnn.benchmark = True
plt.ion()

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# dictionary of channel means and std. devs. for normalization
datasets = {'ORIGA': {'means': [0.8838386, 0.54816394, 0.26245996], 'stds': [0.0874888, 0.11906112, 0.09120588]},
            'REFUGE': {'means': [0.66341796, 0.43719301, 0.29155903], 'stds': [0.16775985, 0.14277969, 0.09243343]},
            'ACRIMA': {'means': [0.72563578, 0.37745064, 0.09958277], 'stds': [0.09922888, 0.10618329, 0.05055466]},
            'RIM-ONE': {'means': [0.58595025, 0.26142626, 0.14453408], 'stds': [0.11523398, 0.09373888, 0.04623389]},
            'COMBO-1': {'means': [0.74480503, 0.40688095, 0.16887603], 'stds': [0.09887175, 0.10700735, 0.06351543]},
            'COMBO-2': {'means': [0.71806865, 0.4300941, 0.24476887], 'stds': [0.12725383, 0.12211156, 0.08018193]},
            'COMBO-3': {'means': [0.75050978, 0.45117447, 0.22012019], 'stds': [0.12083352, 0.12334915, 0.07821309]},
            'COMBO-4': {'means': [0.66637015, 0.37307097, 0.18786628], 'stds': [0.13073384, 0.11786446, 0.06635228]},
            'FULL': {'means': [0.72008179, 0.41608895, 0.20614389], 'stds': [0.11979813, 0.11787404, 0.07229996]}}

num_cls = 2

# trained model path
path = '/content/drive/MyDrive/ORF 498/trained_models/ResNet101_ORIGA_Nov29.pt'

# load trained model
model_ft = models.resnet101()
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, num_cls)
model_ft.load_state_dict(torch.load(path))
model_ft.eval()

# function to test model and get predictions and labels
def collect_preds(model, test_loader):
    was_training = model.training
    # evaluation mode
    model.eval()

    # save in lists
    preds_list = []
    labels_list = []

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # model predicts on minibatch
            outputs = model(inputs)
            # choose class with maximum score
            _, preds = torch.max(outputs, 1)

            # add predictions and labels to full list
            preds_list += preds.tolist()
            labels_list += labels.tolist()
        model.train(mode=was_training)

    return preds_list, labels_list

# function to test model and get AUC score, ROC metrics
def collect_auc_roc(model, test_loader):
    was_training = model.training
    # evaluation mode
    model.eval()

    # save as tensors
    labels_list = torch.tensor(())
    softmax_list = torch.tensor(())

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(test_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # model predicts on minibatch
            outputs = model(inputs)
            # convert predictions to softmax
            softmaxs = F.softmax(outputs, dim=1)

            # add softmax and labels to full tensor
            softmax_list = torch.cat((softmax_list, softmaxs), 0)
            labels_list = torch.cat((labels_list, labels), 0)

        model.train(mode=was_training)

    # calculate AUC score, FPR and TPR rates for ROC curve from labels and softmax predictions
    score = roc_auc_score(labels_list, softmax_list[:,1])
    fpr, tpr, _ = roc_curve(labels_list, softmax_list[:,1])

    return score, fpr, tpr

# function to get confusion matrix metrics from predictions and labels
def acc_and_confusion(preds_list, labels_list):
    tp = 0
    tn = 0
    fp = 0
    fn = 0
    total = len(preds_list)

    for i in range(len(preds_list)):
        # correct class
        if preds_list[i] == labels_list[i]:
            if preds_list[i] == 1 and labels_list[i] == 1: tp += 1
            elif preds_list[i] == 0 and labels_list[i] == 0: tn += 1
            # ensure all predictions are sorted
            else: print("equal not true")

        # incorrect class
        if preds_list[i] != labels_list[i]:
            if preds_list[i] == 1 and labels_list[i] == 0: fp += 1
            elif preds_list[i] == 0 and labels_list[i] == 1: fn += 1
            # ensure all predictions are sorted
            else: print("unequal not true")

    # calculate accuracy
    acc = (tp + tn) / total
    assert total == tp+tn+fp+fn

    return acc, tp, tn, fp, fn

# TEST ON ALL DATASETS
test_sets = ['ORIGA', 'REFUGE', 'ACRIMA', 'RIM-ONE']
dataset = 'COMBO-2'

for test_set in test_sets:
  test_dir = '/content/drive/MyDrive/ORF 498/' + test_set + '_ROI/test/'

  # load and normalize test set by training means and std. devs.
  test_data = ImageFolder(test_dir, transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=datasets[dataset]['means'], std=datasets[dataset]['stds'])]))
  test_loader = DataLoader(dataset=test_data, batch_size = 32, shuffle=True)

  # collect predictions and labels
  preds_list, labels_list = collect_preds(model_ft, test_loader)

  # return and print accuracy and confusion matrix scores
  acc, tp, tn, fp, fn = acc_and_confusion(preds_list, labels_list)
  print("Loading for " + test_set)
  print("Accuracy: ", acc)
  print("TP: ", tp)
  print("TN: ", tn)
  print("FP: ", fp)
  print("FN: ", fn)

# AUC PLOT FOR ALL DATASETS
test_sets = ['ORIGA', 'REFUGE', 'ACRIMA', 'RIM-ONE']
colors = ['orange', 'red', 'green', 'blue']
dataset = 'COMBO-2'

# create empty figure
plt.figure()
i = 0

for test_set in test_sets:
  test_dir = '/content/drive/MyDrive/ORF 498/' + test_set + '_ROI/test/'

  # load and normalize test set by training means and std. devs.
  test_data = ImageFolder(test_dir, transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=datasets[dataset]['means'], std=datasets[dataset]['stds'])]))
  test_loader = DataLoader(dataset=test_data, batch_size = 32, shuffle=True)

  # collect softmax and labels, and return AUC score and ROC measures
  score, fpr, tpr = collect_softmax(model_ft, test_loader)

  # print AUC score for each test set
  print(test_set + " AUC score: " + str(score))

  # add ROC curve to plot for each test set
  plt.plot(fpr, tpr, label=test_set + " (AUC = " + str(round(score,2)) + ")", color=colors[i])
  i += 1

# show and save plot with all 4 test set curves
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves for " + dataset + "-Trained Model")
plt.legend()
plt.savefig('/content/drive/MyDrive/ORF 498/' + dataset + '_ROC_curve.png')
plt.show()